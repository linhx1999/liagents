"""RLè®­ç»ƒ

æä¾›å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬SFTã€GRPOã€PPOç­‰ç®—æ³•ã€‚
ç°åœ¨é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå°†ä¸åŒåŠŸèƒ½åˆ†ç¦»åˆ°ç‹¬ç«‹çš„ç»„ä»¶ä¸­ã€‚
"""

from typing import Dict, Any
import json
from .core import RLTrainingCore
from .handler.data_handler import RLDataHandler
from .handler.reward_handler import RLRewardHandler
from .handler.evaluation_handler import RLEvaluationHandler


class RLTrainer:
    """RLè®­ç»ƒå·¥å…· - ä¸»è¦å…¥å£ç‚¹

    æ”¯æŒçš„è®­ç»ƒç®—æ³•ï¼š
    - SFT: Supervised Fine-Tuning (ç›‘ç£å¾®è°ƒ)
    - GRPO: Group Relative Policy Optimization (ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–)

    æ”¯æŒçš„åŠŸèƒ½ï¼š
    - è®­ç»ƒæ¨¡å‹ (train)
    - åŠ è½½æ•°æ®é›† (load_dataset)
    - åˆ›å»ºå¥–åŠ±å‡½æ•° (create_reward)
    - è¯„ä¼°æ¨¡å‹ (evaluate)
    """

    def __init__(self):
        self.training_core = RLTrainingCore()
        self.data_handler = RLDataHandler()
        self.reward_handler = RLRewardHandler()
        self.evaluation_handler = RLEvaluationHandler()

    def train(self, parameters: Dict[str, Any]) -> str:
        """è®­ç»ƒæ¨¡å‹

        Args:
            parameters: è®­ç»ƒå‚æ•°ï¼ŒåŒ…å«:
                - algorithm: è®­ç»ƒç®—æ³• (sft/grpo)
                - model_name: æ¨¡å‹åç§°
                - dataset: æ•°æ®é›†åç§°
                - num_epochs: è®­ç»ƒè½®æ•°
                - output_dir: è¾“å‡ºç›®å½•
                - use_lora: æ˜¯å¦ä½¿ç”¨LoRA
                - batch_size: æ‰¹æ¬¡å¤§å°
        """
        algorithm = parameters.get("algorithm", "sft").lower()
        model_name = parameters.get("model_name", "Qwen/Qwen2-0.5B-Instruct")
        dataset_name = parameters.get("dataset", "gsm8k")
        max_samples = parameters.get("max_samples", None)
        num_epochs = parameters.get("num_epochs", 3)
        output_dir = parameters.get("output_dir", "./output")
        use_lora = parameters.get("use_lora", True)
        batch_size = parameters.get("batch_size", 4)
        custom_dataset = parameters.get("custom_dataset", None)
        custom_reward = parameters.get("custom_reward", None)
        use_wandb = parameters.get("use_wandb", False)
        use_tensorboard = parameters.get("use_tensorboard", True)
        wandb_project = parameters.get("wandb_project", None)

        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹ {algorithm.upper()} è®­ç»ƒ")
        print(f"{'='*60}")
        print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
        if custom_dataset:
            print(f"ğŸ“Š æ•°æ®é›†: è‡ªå®šä¹‰æ•°æ®é›†")
        else:
            print(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
        print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¯ ç®—æ³•: {algorithm.upper()}")
        if custom_reward:
            print(f"ğŸ å¥–åŠ±å‡½æ•°: è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°")

        monitoring = []
        if use_wandb:
            monitoring.append(f"wandb (é¡¹ç›®: {wandb_project or 'default'})")
        if use_tensorboard:
            monitoring.append("tensorboard")
        if monitoring:
            print(f"ğŸ“Š è®­ç»ƒç›‘æ§: {', '.join(monitoring)}")

        print(f"{'='*60}\n")

        if not self.training_core.trl_available:
            return json.dumps({
                "status": "error",
                "message": "TRLæœªå®‰è£…"
            }, ensure_ascii=False, indent=2)

        if algorithm == "sft":
            result = self.training_core.train_sft(
                model_name=model_name,
                dataset_name=dataset_name,
                max_samples=max_samples,
                num_epochs=num_epochs,
                output_dir=output_dir,
                use_lora=use_lora,
                batch_size=batch_size,
                custom_dataset=custom_dataset,
                use_wandb=use_wandb,
                use_tensorboard=use_tensorboard,
                wandb_project=wandb_project
            )
        elif algorithm == "grpo":
            result = self.training_core.train_grpo(
                model_name=model_name,
                dataset_name=dataset_name,
                max_samples=max_samples,
                num_epochs=num_epochs,
                output_dir=output_dir,
                use_lora=use_lora,
                batch_size=batch_size,
                custom_dataset=custom_dataset,
                custom_reward=custom_reward,
                use_wandb=use_wandb,
                use_tensorboard=use_tensorboard,
                wandb_project=wandb_project
            )
        else:
            result = {
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}ã€‚æ”¯æŒçš„ç®—æ³•: sft, grpo"
            }

        return json.dumps(result, ensure_ascii=False, indent=2)

    def load_dataset(self, parameters: Dict[str, Any]) -> str:
        """åŠ è½½æ•°æ®é›†"""
        return self.data_handler.handle_load_dataset(parameters)

    def create_reward(self, parameters: Dict[str, Any]) -> str:
        """åˆ›å»ºå¥–åŠ±å‡½æ•°"""
        return self.reward_handler.handle_create_reward(parameters)

    def evaluate(self, parameters: Dict[str, Any]) -> str:
        """è¯„ä¼°æ¨¡å‹"""
        return self.evaluation_handler.handle_evaluate(parameters)

    # ä¾¿æ·å‡½æ•°æ¥å£
    def register_dataset(self, name: str, dataset) -> None:
        """
        æ³¨å†Œè‡ªå®šä¹‰æ•°æ®é›†

        Args:
            name: æ•°æ®é›†åç§°
            dataset: æ•°æ®é›†å¯¹è±¡(HuggingFace Dataset)
        """
        self.data_handler.register_dataset(name, dataset)

    def register_reward_function(self, name: str, reward_fn) -> None:
        """
        æ³¨å†Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

        Args:
            name: å¥–åŠ±å‡½æ•°åç§°
            reward_fn: å¥–åŠ±å‡½æ•°(æ¥å—completionså’Œkwargs,è¿”å›rewardsåˆ—è¡¨)
        """
        self.reward_handler.register_reward_function(name, reward_fn)