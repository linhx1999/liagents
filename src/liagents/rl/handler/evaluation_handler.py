"""RLæ¨¡å‹è¯„ä¼°æ¨¡å—

åŒ…å«æ¨¡å‹è¯„ä¼°ç›¸å…³çš„åŠŸèƒ½ã€‚
"""

from typing import Dict, Any
import json


class RLEvaluationHandler:
    """RLè¯„ä¼°å¤„ç†ç±»ï¼Œè´Ÿè´£æ¨¡å‹è¯„ä¼°åŠŸèƒ½"""

    def handle_evaluate(self, parameters: Dict[str, Any]) -> str:
        """å¤„ç†æ¨¡å‹è¯„ä¼°æ“ä½œ"""
        try:
            from hello_agents.rl import (
                create_rl_dataset,
                create_accuracy_reward,
                evaluate_rewards
            )
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            model_path = parameters.get("model_path")
            max_samples = parameters.get("max_samples", 100)

            if not model_path:
                return json.dumps({
                    "status": "error",
                    "message": "ç¼ºå°‘å¿…éœ€å‚æ•°: model_path"
                }, ensure_ascii=False, indent=2)

            # åŠ è½½æµ‹è¯•æ•°æ®
            print(f"ğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ®é›† (max_samples={max_samples})...")
            dataset = create_rl_dataset(split="test", max_samples=max_samples, model_name=model_path)

            # åŠ è½½æ¨¡å‹å’Œtokenizer
            print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}...")
            try:
                model = AutoModelForCausalLM.from_pretrained(model_path)
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                device = "cuda" if torch.cuda.is_available() else "cpu"
                model = model.to(device)
                model.eval()
            except Exception as e:
                return json.dumps({
                    "status": "error",
                    "message": f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
                }, ensure_ascii=False, indent=2)

            # ç”Ÿæˆé¢„æµ‹
            print("ğŸ”® ç”Ÿæˆé¢„æµ‹...")
            completions = []
            ground_truths = []

            # å¯¼å…¥tqdmç”¨äºè¿›åº¦æ¡
            try:
                from tqdm import tqdm
                use_tqdm = True
            except ImportError:
                use_tqdm = False
                print("  æç¤º: å®‰è£…tqdmå¯æ˜¾ç¤ºè¿›åº¦æ¡ (pip install tqdm)")

            # åˆ›å»ºè¿­ä»£å™¨
            iterator = range(min(max_samples, len(dataset)))
            if use_tqdm:
                iterator = tqdm(iterator, desc="  è¯„ä¼°è¿›åº¦", unit="æ ·æœ¬")

            for i in iterator:
                prompt = dataset[i]["prompt"]
                ground_truth = dataset[i]["ground_truth"]

                # ç”Ÿæˆå›ç­”
                inputs = tokenizer(prompt, return_tensors="pt").to(device)
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=128,  # å‡å°‘ç”Ÿæˆé•¿åº¦åŠ å¿«é€Ÿåº¦
                        temperature=0.7,
                        do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç åŠ å¿«é€Ÿåº¦
                        pad_token_id=tokenizer.pad_token_id
                    )
                # åªå–ç”Ÿæˆçš„éƒ¨åˆ†,ä¸åŒ…æ‹¬è¾“å…¥
                completion = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

                completions.append(completion)
                ground_truths.append(ground_truth)

                # å¦‚æœæ²¡æœ‰tqdm,æ¯10ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
                if not use_tqdm and (i + 1) % 10 == 0:
                    print(f"  è¿›åº¦: {i+1}/{max_samples}")

            # è®¡ç®—å¥–åŠ±
            print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
            reward_fn = create_accuracy_reward()
            rewards = reward_fn(completions, ground_truth=ground_truths)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_reward = sum(rewards) / len(rewards)
            accuracy = avg_reward  # å¯¹äºå‡†ç¡®æ€§å¥–åŠ±,å¹³å‡å¥–åŠ±å°±æ˜¯å‡†ç¡®ç‡

            result = {
                "status": "success",
                "model_path": model_path,
                "num_samples": len(completions),
                "accuracy": f"{accuracy:.2%}",
                "average_reward": f"{avg_reward:.4f}",
                "device": device
            }

            print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
            print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
            print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")

            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            return json.dumps({
                "status": "error",
                "message": f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }, ensure_ascii=False, indent=2)